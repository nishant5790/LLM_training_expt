{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# LoRA & PEFT LLM Training Tutorial\n",
        "\n",
        "## 🎯 Complete Guide to Parameter-Efficient Fine-Tuning\n",
        "\n",
        "This tutorial will walk you through the entire process of fine-tuning a Large Language Model using **LoRA (Low-Rank Adaptation)** and the **PEFT (Parameter-Efficient Fine-Tuning)** library.\n",
        "\n",
        "### What You'll Learn:\n",
        "- 🧠 **Understanding LoRA**: How it works and why it's efficient\n",
        "- 🔧 **Setting up models**: Loading and configuring pre-trained models\n",
        "- 📊 **Data preprocessing**: Formatting data for instruction tuning\n",
        "- 🚀 **Training**: Fine-tuning with LoRA adapters\n",
        "- 🔍 **Evaluation**: Testing model performance\n",
        "- 💡 **Inference**: Using your trained model\n",
        "\n",
        "### Prerequisites:\n",
        "- Basic Python knowledge\n",
        "- Understanding of neural networks and transformers\n",
        "- Familiarity with PyTorch and Hugging Face libraries\n",
        "\n",
        "Let's get started! 🚀\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📚 Part 1: Understanding LoRA and PEFT\n",
        "\n",
        "### What is LoRA (Low-Rank Adaptation)?\n",
        "\n",
        "LoRA is a technique that allows us to fine-tune large language models efficiently by:\n",
        "\n",
        "1. **Freezing the original model weights** - The pre-trained parameters remain unchanged\n",
        "2. **Adding trainable low-rank matrices** - Small matrices A and B are added to attention layers\n",
        "3. **Decomposing weight updates** - Instead of updating the full weight matrix W, we learn ΔW = BA where B and A are much smaller matrices\n",
        "\n",
        "### Mathematical Foundation:\n",
        "```\n",
        "Original: W + ΔW  (where ΔW is full rank)\n",
        "LoRA:     W + BA  (where B ∈ R^(d×r), A ∈ R^(r×k), r << d,k)\n",
        "```\n",
        "\n",
        "### Benefits of LoRA:\n",
        "- 🔥 **Memory Efficient**: Only 0.1-3% of original parameters are trainable\n",
        "- ⚡ **Faster Training**: Reduced computational requirements\n",
        "- 💾 **Smaller Checkpoints**: Only adapter weights need to be saved\n",
        "- 🔄 **Modular**: Multiple adapters can be trained for different tasks\n",
        "- 🛡️ **Preserves Base Model**: Original model remains intact\n",
        "\n",
        "### PEFT Library:\n",
        "The PEFT library provides implementations of various parameter-efficient methods:\n",
        "- **LoRA** - Low-Rank Adaptation\n",
        "- **AdaLoRA** - Adaptive LoRA with dynamic rank allocation\n",
        "- **Prefix Tuning** - Adding trainable prefix tokens\n",
        "- **P-Tuning v2** - Prompt tuning methods\n",
        "- **IA³** - Infused Adapter by Inhibiting and Amplifying Inner Activations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path for our custom modules\n",
        "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"GPU not available, using CPU\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🔧 Part 2: Setting Up the Model and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for our LoRA training\n",
        "config = {\n",
        "    \"model\": {\n",
        "        \"model_name\": \"microsoft/DialoGPT-medium\",  # Using a smaller model for tutorial\n",
        "        \"cache_dir\": \"../model_cache\",\n",
        "        \"torch_dtype\": \"auto\"\n",
        "    },\n",
        "    \"lora\": {\n",
        "        \"r\": 16,                    # Rank of adaptation\n",
        "        \"lora_alpha\": 32,           # LoRA scaling parameter\n",
        "        \"target_modules\": [         # Which modules to apply LoRA to\n",
        "            \"c_attn\",               # Attention projections\n",
        "            \"c_proj\", \n",
        "            \"c_fc\"                  # Feed-forward layers\n",
        "        ],\n",
        "        \"lora_dropout\": 0.1,        # Dropout for LoRA layers\n",
        "        \"bias\": \"none\",             # Whether to adapt bias parameters\n",
        "        \"task_type\": \"CAUSAL_LM\"    # Type of task\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"data_path\": \"../data/sample_data.json\",\n",
        "        \"max_seq_length\": 512,\n",
        "        \"train_on_inputs\": False,   # Only train on assistant responses\n",
        "        \"validation_split\": 0.1\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"output_dir\": \"./outputs\",\n",
        "        \"num_train_epochs\": 1,      # Reduced for tutorial\n",
        "        \"per_device_train_batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"warmup_steps\": 50,\n",
        "        \"logging_steps\": 10,\n",
        "        \"save_steps\": 100,\n",
        "        \"eval_steps\": 100,\n",
        "        \"bf16\": True,               # Use mixed precision\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"report_to\": [],            # Disable wandb for tutorial\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and setup the model with LoRA adapters\n",
        "from model_setup import ModelSetup\n",
        "from data_preprocessing import DataProcessor, ChatDataCollator\n",
        "\n",
        "# Initialize model setup\n",
        "print(\"Setting up model...\")\n",
        "model_setup = ModelSetup(config)\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = model_setup.setup_model_and_tokenizer()\n",
        "\n",
        "# Print model information\n",
        "def print_model_info(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"MODEL INFORMATION\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "print_model_info(model)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📊 Part 3: Data Preparation\n",
        "\n",
        "Now let's load and prepare our instruction-following dataset for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup data processor\n",
        "data_processor = DataProcessor(\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=config[\"data\"][\"max_seq_length\"],\n",
        "    train_on_inputs=config[\"data\"][\"train_on_inputs\"]\n",
        ")\n",
        "\n",
        "# Load and prepare datasets\n",
        "print(\"Loading datasets...\")\n",
        "datasets = data_processor.prepare_datasets(\n",
        "    dataset_name=\"local\",\n",
        "    data_path=config[\"data\"][\"data_path\"],\n",
        "    validation_split=config[\"data\"][\"validation_split\"],\n",
        "    num_proc=4\n",
        ")\n",
        "\n",
        "print(f\"Training examples: {len(datasets['train'])}\")\n",
        "print(f\"Validation examples: {len(datasets['validation'])}\")\n",
        "\n",
        "# Create data collator\n",
        "data_collator = ChatDataCollator(\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=config[\"data\"][\"max_seq_length\"],\n",
        "    train_on_inputs=config[\"data\"][\"train_on_inputs\"]\n",
        ")\n",
        "\n",
        "# Let's look at a sample\n",
        "sample_example = datasets[\"train\"][0]\n",
        "print(\"\\nSample tokenized example:\")\n",
        "print(f\"Input IDs length: {len(sample_example['input_ids'])}\")\n",
        "print(f\"Labels length: {len(sample_example['labels'])}\")\n",
        "\n",
        "# Decode to see the actual text\n",
        "decoded_text = tokenizer.decode(sample_example['input_ids'])\n",
        "print(f\"\\nDecoded text: {decoded_text[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🚀 Part 4: Training the Model\n",
        "\n",
        "Now let's train our model using LoRA adapters!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup trainer\n",
        "from trainer import LoRATrainer\n",
        "\n",
        "print(\"Setting up trainer...\")\n",
        "trainer = LoRATrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "training_results = trainer.train(\n",
        "    train_dataset=datasets[\"train\"].select(range(50)),  # Use subset for demo\n",
        "    eval_dataset=datasets[\"validation\"].select(range(20)),\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"Final training loss: {training_results.get('train_loss', 'N/A')}\")\n",
        "print(f\"Training time: {training_results.get('train_runtime', 'N/A'):.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 💡 Part 5: Testing the Trained Model\n",
        "\n",
        "Let's test our newly trained LoRA model with some example prompts!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup inference\n",
        "from inference import LoRAInference\n",
        "\n",
        "inference = LoRAInference(model, tokenizer, config)\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Explain machine learning in simple terms.\",\n",
        "    \"What are the benefits of renewable energy?\",\n",
        "    \"How does photosynthesis work?\",\n",
        "    \"Write a short story about a robot learning to paint.\"\n",
        "]\n",
        "\n",
        "print(\"🤖 Testing the trained model:\\n\" + \"=\"*60)\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n[{i}] Prompt: {prompt}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    response = inference.chat(prompt)\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 🎉 Conclusion\n",
        "\n",
        "Congratulations! You've successfully:\n",
        "\n",
        "✅ **Understood LoRA**: Learned how low-rank adaptation works  \n",
        "✅ **Configured PEFT**: Set up parameter-efficient fine-tuning  \n",
        "✅ **Prepared Data**: Formatted instruction-following datasets  \n",
        "✅ **Trained Model**: Fine-tuned with LoRA adapters  \n",
        "✅ **Tested Results**: Evaluated your trained model  \n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Efficiency**: LoRA reduces trainable parameters by 99%+ while maintaining performance\n",
        "2. **Memory**: Significantly lower GPU memory requirements\n",
        "3. **Speed**: Faster training and inference\n",
        "4. **Modularity**: Easy to swap different adapters for different tasks\n",
        "5. **Preservation**: Original model weights remain intact\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- 🔄 **Experiment with hyperparameters**: Try different ranks, alpha values, and target modules\n",
        "- 📊 **Use larger datasets**: Scale up to thousands or millions of examples  \n",
        "- 🎯 **Task-specific training**: Create adapters for specific domains (medical, legal, etc.)\n",
        "- 🔗 **Multi-adapter setups**: Combine multiple LoRA adapters\n",
        "- 🚀 **Deploy in production**: Use your trained models in applications\n",
        "\n",
        "### Resources:\n",
        "\n",
        "- 📖 [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
        "- 🛠️ [PEFT Documentation](https://huggingface.co/docs/peft)\n",
        "- 💻 [Hugging Face Transformers](https://huggingface.co/docs/transformers)\n",
        "\n",
        "Happy fine-tuning! 🚀\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
